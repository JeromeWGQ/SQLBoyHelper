import pandas as pd
from pypinyin import pinyin, Style
from collections import Counter
import re


def read_excel_range_by_columns(file_path, sheet_name, start_cell, end_cell):
    """
    è¯»å–ExcelæŒ‡å®šåŒºåŸŸæ•°æ®ï¼ŒæŒ‰çºµå‘ï¼ˆåˆ—ï¼‰é¡ºåºå­˜å‚¨åˆ°æ•°ç»„ä¸­

    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°
        start_cell: èµ·å§‹å•å…ƒæ ¼ (å¦‚ 'B2')
        end_cell: ç»“æŸå•å…ƒæ ¼ (å¦‚ 'Y25')

    Returns:
        list: æŒ‰åˆ—é¡ºåºè¯»å–çš„æ‰€æœ‰æ•°æ®
    """
    try:
        # ä½¿ç”¨pandasè¯»å–
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)

        # è§£æèµ·å§‹å’Œç»“æŸä½ç½®
        start_col = ord(start_cell[0]) - ord('A')  # Båˆ—å¯¹åº”ç´¢å¼•1
        start_row = int(start_cell[1:]) - 1  # ç¬¬2è¡Œå¯¹åº”ç´¢å¼•1
        end_col = ord(end_cell[0]) - ord('A')  # Yåˆ—å¯¹åº”ç´¢å¼•24
        end_row = int(end_cell[1:]) - 1  # ç¬¬25è¡Œå¯¹åº”ç´¢å¼•24

        # æˆªå–æŒ‡å®šåŒºåŸŸ
        region_data = df.iloc[start_row:end_row + 1, start_col:end_col + 1]

        # æŒ‰åˆ—é¡ºåºè¯»å–æ•°æ®åˆ°ä¸€ç»´æ•°ç»„
        result_array = []
        for col in range(region_data.shape[1]):  # éå†æ¯ä¸€åˆ—
            column_data = region_data.iloc[:, col].tolist()
            result_array.extend(column_data)

        # è¿‡æ»¤æ‰ç©ºå€¼ï¼ˆNaNï¼‰å’Œ"-"
        result_array = [str(item) for item in result_array if pd.notna(item) and str(item) != "-"]

        return result_array

    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶å‡ºé”™: {e}")
        return []


def classify_words(words):
    """
    å¯¹è¯æ±‡è¿›è¡Œåˆ†ç±»ï¼šæ­£å¸¸æ±‰å­—è¯æ±‡ã€è‹±æ–‡è¯æ±‡ã€å•å­—è¯æ±‡ã€å…¶ä»–ç‰¹æ®Šè¯æ±‡

    Args:
        words: è¯è¯­åˆ—è¡¨

    Returns:
        dict: åˆ†ç±»ç»“æœ
    """
    classification = {
        'normal_chinese': [],  # æ­£å¸¸æ±‰å­—è¯æ±‡ï¼ˆå¤šå­—ï¼‰
        'single_chinese': [],  # å•ä¸ªæ±‰å­—è¯æ±‡
        'english_words': [],  # è‹±æ–‡è¯æ±‡
        'mixed_words': [],  # ä¸­è‹±æ··åˆè¯æ±‡
        'special_words': []  # å…¶ä»–ç‰¹æ®Šè¯æ±‡ï¼ˆæ•°å­—ã€ç¬¦å·ç­‰ï¼‰
    }

    for word in words:
        if len(word) == 1:
            # å•å­—è¯æ±‡
            if '\u4e00' <= word <= '\u9fff':
                classification['single_chinese'].append(word)
            elif word.isalpha() and word.isascii():
                classification['english_words'].append(word)
            else:
                classification['special_words'].append(word)

        elif len(word) > 1:
            # å¤šå­—è¯æ±‡
            chinese_chars = sum(1 for char in word if '\u4e00' <= char <= '\u9fff')
            english_chars = sum(1 for char in word if char.isalpha() and char.isascii())
            other_chars = len(word) - chinese_chars - english_chars

            if chinese_chars == len(word):
                # çº¯æ±‰å­—è¯æ±‡
                classification['normal_chinese'].append(word)
            elif english_chars == len(word):
                # çº¯è‹±æ–‡è¯æ±‡
                classification['english_words'].append(word)
            elif chinese_chars > 0 and english_chars > 0:
                # ä¸­è‹±æ··åˆè¯æ±‡
                classification['mixed_words'].append(word)
            else:
                # å…¶ä»–ç‰¹æ®Šè¯æ±‡
                classification['special_words'].append(word)

    return classification


def get_char_pinyin(word, char_index=0):
    """
    è·å–è¯è¯­æŒ‡å®šä½ç½®å­—ç¬¦çš„æ‹¼éŸ³ï¼ˆæ•°å­—å£°è°ƒå½¢å¼ï¼‰

    Args:
        word: è¯è¯­å­—ç¬¦ä¸²
        char_index: å­—ç¬¦ç´¢å¼•ä½ç½®ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€ä¸ªå­—ï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªå­—ï¼‰

    Returns:
        str: æŒ‡å®šå­—ç¬¦çš„æ‹¼éŸ³ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
    """
    if not word or len(word) <= char_index:
        return None

    try:
        # è·å–æŒ‡å®šä½ç½®çš„å­—ç¬¦
        target_char = word[char_index]

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ±‰å­—
        if '\u4e00' <= target_char <= '\u9fff':
            # è·å–æ‹¼éŸ³ï¼ˆæ•°å­—å£°è°ƒå½¢å¼ï¼‰
            pinyin_result = pinyin(target_char, style=Style.TONE2)
            if pinyin_result and len(pinyin_result[0]) > 0:
                return pinyin_result[0][0]

        return None

    except Exception as e:
        print(f"è·å–æ‹¼éŸ³å‡ºé”™ '{word}' ç¬¬{char_index + 1}ä¸ªå­—: {e}")
        return None


def get_char(word, char_index=0):
    """
    è·å–è¯è¯­æŒ‡å®šä½ç½®çš„å­—ç¬¦ï¼ˆæ±‰å­—ï¼‰

    Args:
        word: è¯è¯­å­—ç¬¦ä¸²
        char_index: å­—ç¬¦ç´¢å¼•ä½ç½®

    Returns:
        str: æŒ‡å®šä½ç½®çš„æ±‰å­—ï¼Œå¦‚æœä¸æ˜¯æ±‰å­—è¿”å›None
    """
    if not word or len(word) <= char_index:
        return None

    target_char = word[char_index]

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ±‰å­—
    if '\u4e00' <= target_char <= '\u9fff':
        return target_char

    return None


def analyze_special_words(classification):
    """
    åˆ†æç‰¹æ®Šè¯æ±‡çš„è¯»éŸ³æƒ…å†µ

    Args:
        classification: è¯æ±‡åˆ†ç±»ç»“æœ

    Returns:
        dict: ç‰¹æ®Šè¯æ±‡åˆ†æç»“æœ
    """
    special_analysis = {
        'single_chinese_analysis': [],
        'english_words_analysis': [],
        'mixed_words_analysis': [],
        'other_special_analysis': []
    }

    # åˆ†æå•å­—æ±‰å­—è¯æ±‡
    for word in classification['single_chinese']:
        pinyin = get_char_pinyin(word, 0)
        special_analysis['single_chinese_analysis'].append({
            'word': word,
            'pronunciation': word,  # å•å­—ç›´æ¥ä½¿ç”¨è‡ªèº«ä½œä¸ºè¯»éŸ³
            'pinyin': pinyin,
            'reason': 'å•å­—æ±‰å­—ï¼Œç›´æ¥ä½¿ç”¨è‡ªèº«'
        })

    # åˆ†æè‹±æ–‡è¯æ±‡
    for word in classification['english_words']:
        special_analysis['english_words_analysis'].append({
            'word': word,
            'pronunciation': word,  # è‹±æ–‡è¯æ±‡ä½¿ç”¨è‡ªèº«
            'pinyin': None,
            'reason': 'è‹±æ–‡è¯æ±‡ï¼Œä½¿ç”¨åŸè¯'
        })

    # åˆ†æä¸­è‹±æ··åˆè¯æ±‡
    for word in classification['mixed_words']:
        # å°è¯•æå–æ±‰å­—éƒ¨åˆ†çš„æ‹¼éŸ³
        chinese_chars = [char for char in word if '\u4e00' <= char <= '\u9fff']
        if chinese_chars:
            first_chinese_pinyin = get_char_pinyin(chinese_chars[0], 0) if chinese_chars else None
            special_analysis['mixed_words_analysis'].append({
                'word': word,
                'pronunciation': word,  # æ··åˆè¯æ±‡ä½¿ç”¨å®Œæ•´è¯
                'pinyin': first_chinese_pinyin,
                'chinese_part': ''.join(chinese_chars),
                'reason': 'ä¸­è‹±æ··åˆè¯æ±‡ï¼Œä½¿ç”¨å®Œæ•´è¯'
            })
        else:
            special_analysis['mixed_words_analysis'].append({
                'word': word,
                'pronunciation': word,
                'pinyin': None,
                'chinese_part': '',
                'reason': 'ä¸­è‹±æ··åˆè¯æ±‡ï¼Œä½¿ç”¨å®Œæ•´è¯'
            })

    # åˆ†æå…¶ä»–ç‰¹æ®Šè¯æ±‡
    for word in classification['special_words']:
        special_analysis['other_special_analysis'].append({
            'word': word,
            'pronunciation': word,  # ç‰¹æ®Šè¯æ±‡ä½¿ç”¨è‡ªèº«
            'pinyin': None,
            'reason': 'ç‰¹æ®Šè¯æ±‡ï¼ˆæ•°å­—/ç¬¦å·ç­‰ï¼‰ï¼Œä½¿ç”¨åŸè¯'
        })

    return special_analysis


# ==================== ç¬¬ä¸€ç±»æ–¹æ¡ˆ ====================

def determine_word_pronunciation_method1(words):
    """
    ç¬¬ä¸€ç±»æ–¹æ¡ˆï¼šæ ¹æ®é¦–å­—è¯»éŸ³çš„ç‹¬ç‰¹æ€§ç¡®å®šè¯»éŸ³è¡¨ç¤º
    åªå¤„ç†æ­£å¸¸çš„å¤šå­—æ±‰å­—è¯æ±‡
    """
    print("ğŸ”¤ ã€ç¬¬ä¸€ç±»æ–¹æ¡ˆã€‘æ­£åœ¨è·å–æ‰€æœ‰è¯çš„é¦–å­—è¯»éŸ³...")

    # å…ˆè¿›è¡Œè¯æ±‡åˆ†ç±»
    classification = classify_words(words)
    normal_words = classification['normal_chinese']

    print(f"ğŸ“Š è¯æ±‡åˆ†ç±»ç»Ÿè®¡:")
    print(f"   æ­£å¸¸æ±‰å­—è¯æ±‡: {len(normal_words)}")
    print(f"   å•å­—æ±‰å­—è¯æ±‡: {len(classification['single_chinese'])}")
    print(f"   è‹±æ–‡è¯æ±‡: {len(classification['english_words'])}")
    print(f"   ä¸­è‹±æ··åˆè¯æ±‡: {len(classification['mixed_words'])}")
    print(f"   å…¶ä»–ç‰¹æ®Šè¯æ±‡: {len(classification['special_words'])}")

    # ç¬¬ä¸€æ­¥ï¼šè·å–æ­£å¸¸è¯æ±‡çš„é¦–å­—è¯»éŸ³å’Œé¦–å­—
    word_info = []
    for word in normal_words:
        first_char = get_char(word, 0)
        first_char_pinyin = get_char_pinyin(word, 0)

        word_info.append({
            'word': word,
            'first_char': first_char,
            'first_char_pinyin': first_char_pinyin
        })

    # è¿‡æ»¤æ‰æ— æ³•è·å–æ‹¼éŸ³çš„è¯
    valid_word_info = [info for info in word_info if info['first_char_pinyin'] is not None]

    print(f"âœ… æˆåŠŸè·å– {len(valid_word_info)} ä¸ªæ­£å¸¸è¯è¯­çš„é¦–å­—è¯»éŸ³")

    # ç¬¬äºŒæ­¥ï¼šç»Ÿè®¡é¦–å­—è¯»éŸ³çš„é‡å¤æƒ…å†µ
    print("ğŸ“Š æ­£åœ¨ç»Ÿè®¡é¦–å­—è¯»éŸ³é‡å¤æƒ…å†µ...")

    pinyin_counter = Counter()
    for info in valid_word_info:
        pinyin_counter[info['first_char_pinyin']] += 1

    # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®é€»è¾‘ç¡®å®šæ¯ä¸ªè¯çš„è¯»éŸ³è¡¨ç¤º
    print("ğŸ¯ æ­£åœ¨ç¡®å®šæ¯ä¸ªè¯çš„è¯»éŸ³è¡¨ç¤º...")

    pronunciation_results = []

    for info in valid_word_info:
        word = info['word']
        first_char = info['first_char']
        first_char_pinyin = info['first_char_pinyin']

        # æ£€æŸ¥é¦–å­—è¯»éŸ³æ˜¯å¦ç‹¬ä¸€æ— äºŒ
        if pinyin_counter[first_char_pinyin] == 1:
            # ç‹¬ä¸€æ— äºŒï¼Œä½¿ç”¨é¦–å­—
            pronunciation = first_char
            reason = "é¦–å­—è¯»éŸ³ç‹¬ä¸€æ— äºŒ"
            is_unique = True
        else:
            # æœ‰é‡å¤ï¼Œä½¿ç”¨å®Œæ•´è¯è¯­
            pronunciation = word
            reason = f"é¦–å­—è¯»éŸ³é‡å¤({pinyin_counter[first_char_pinyin]}æ¬¡)"
            is_unique = False

        pronunciation_results.append({
            'word': word,
            'first_char': first_char,
            'first_char_pinyin': first_char_pinyin,
            'pronunciation': pronunciation,
            'is_unique': is_unique,
            'reason': reason,
            'repeat_count': pinyin_counter[first_char_pinyin]
        })

    # ç»Ÿè®¡ä¿¡æ¯
    total_words = len(pronunciation_results)
    unique_count = sum(1 for result in pronunciation_results if result['is_unique'])
    repeated_count = total_words - unique_count
    unique_pinyin_count = len(pinyin_counter)
    repeated_pinyin_count = sum(1 for count in pinyin_counter.values() if count > 1)

    stats = {
        'total_words': total_words,
        'unique_pronunciation_count': unique_count,
        'repeated_pronunciation_count': repeated_count,
        'unique_pinyin_count': unique_pinyin_count,
        'repeated_pinyin_count': repeated_pinyin_count,
        'pinyin_frequency': dict(pinyin_counter)
    }

    return pronunciation_results, stats, classification


# ==================== ç¬¬äºŒç±»æ–¹æ¡ˆ ====================

def determine_word_pronunciation_method2(words):
    """
    ç¬¬äºŒç±»æ–¹æ¡ˆï¼šåŸºäºé¦–å­—å’Œç¬¬äºŒä¸ªå­—è¯»éŸ³è”åˆé›†åˆçš„ç‹¬ç‰¹æ€§ç¡®å®šè¯»éŸ³è¡¨ç¤º
    è”åˆé›†åˆåŒ…å«ï¼šæ­£å¸¸è¯æ±‡çš„é¦–å­—è¯»éŸ³ + æ­£å¸¸è¯æ±‡çš„ç¬¬äºŒä¸ªå­—è¯»éŸ³ + å•å­—è¯æ±‡çš„è¯»éŸ³
    """
    print("ğŸ”¤ ã€ç¬¬äºŒç±»æ–¹æ¡ˆã€‘æ­£åœ¨è·å–æ‰€æœ‰è¯çš„é¦–å­—å’Œç¬¬äºŒä¸ªå­—è¯»éŸ³...")

    # å…ˆè¿›è¡Œè¯æ±‡åˆ†ç±»
    classification = classify_words(words)
    normal_words = classification['normal_chinese']
    single_words = classification['single_chinese']

    print(f"ğŸ“Š è¯æ±‡åˆ†ç±»ç»Ÿè®¡:")
    print(f"   æ­£å¸¸æ±‰å­—è¯æ±‡: {len(normal_words)}")
    print(f"   å•å­—æ±‰å­—è¯æ±‡: {len(single_words)}")

    # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰è¯çš„é¦–å­—å’Œç¬¬äºŒä¸ªå­—ä¿¡æ¯
    word_info = []
    all_first_pinyin = []
    all_second_pinyin = []
    all_single_pinyin = []  # æ–°å¢ï¼šå•å­—è¯æ±‡çš„è¯»éŸ³

    # å¤„ç†æ­£å¸¸è¯æ±‡
    for word in normal_words:
        first_char = get_char(word, 0)
        first_char_pinyin = get_char_pinyin(word, 0)
        second_char = get_char(word, 1)
        second_char_pinyin = get_char_pinyin(word, 1)

        word_info.append({
            'word': word,
            'first_char': first_char,
            'first_char_pinyin': first_char_pinyin,
            'second_char': second_char,
            'second_char_pinyin': second_char_pinyin
        })

        # æ”¶é›†æ‰€æœ‰æ‹¼éŸ³ç”¨äºæ„å»ºè”åˆé›†åˆ
        if first_char_pinyin:
            all_first_pinyin.append(first_char_pinyin)
        if second_char_pinyin:
            all_second_pinyin.append(second_char_pinyin)

    # å¤„ç†å•å­—è¯æ±‡ï¼Œå°†å…¶è¯»éŸ³ä¹ŸåŠ å…¥è”åˆé›†åˆ
    for word in single_words:
        single_pinyin = get_char_pinyin(word, 0)
        if single_pinyin:
            all_single_pinyin.append(single_pinyin)

    # è¿‡æ»¤æ‰æ— æ³•è·å–é¦–å­—æ‹¼éŸ³çš„è¯
    valid_word_info = [info for info in word_info if info['first_char_pinyin'] is not None]

    print(f"âœ… æˆåŠŸè·å– {len(valid_word_info)} ä¸ªæ­£å¸¸è¯è¯­çš„å­—ç¬¦ä¿¡æ¯")
    print(f"âœ… æˆåŠŸè·å– {len(all_single_pinyin)} ä¸ªå•å­—è¯è¯­çš„è¯»éŸ³")

    # ç¬¬äºŒæ­¥ï¼šæ„å»ºè”åˆé›†åˆå¹¶ç»Ÿè®¡ï¼ˆåŒ…å«å•å­—è¯»éŸ³ï¼‰
    print("ğŸ”— æ­£åœ¨æ„å»ºé¦–å­—ã€ç¬¬äºŒä¸ªå­—å’Œå•å­—è¯»éŸ³çš„è”åˆé›†åˆ...")

    # æ„å»ºè”åˆé›†åˆï¼ˆåŒ…å«å•å­—è¯»éŸ³ï¼‰
    union_pinyin_list = all_first_pinyin + all_second_pinyin + all_single_pinyin
    union_pinyin_counter = Counter(union_pinyin_list)

    # åˆ†åˆ«ç»Ÿè®¡å„ç±»è¯»éŸ³
    first_pinyin_counter = Counter(all_first_pinyin)
    second_pinyin_counter = Counter(all_second_pinyin)
    single_pinyin_counter = Counter(all_single_pinyin)

    print(f"ğŸ“Š è”åˆé›†åˆç»Ÿè®¡:")
    print(f"   é¦–å­—è¯»éŸ³æ•°: {len(all_first_pinyin)}")
    print(f"   ç¬¬äºŒä¸ªå­—è¯»éŸ³æ•°: {len(all_second_pinyin)}")
    print(f"   å•å­—è¯»éŸ³æ•°: {len(all_single_pinyin)}")
    print(f"   è”åˆé›†åˆæ€»æ•°: {len(union_pinyin_list)}")
    print(f"   è”åˆé›†åˆå”¯ä¸€è¯»éŸ³æ•°: {len(union_pinyin_counter)}")

    # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®é€»è¾‘ç¡®å®šæ¯ä¸ªè¯çš„è¯»éŸ³è¡¨ç¤º
    print("ğŸ¯ æ­£åœ¨æ ¹æ®è”åˆé›†åˆç¡®å®šæ¯ä¸ªè¯çš„è¯»éŸ³è¡¨ç¤º...")

    pronunciation_results = []

    for info in valid_word_info:
        word = info['word']
        first_char = info['first_char']
        first_char_pinyin = info['first_char_pinyin']
        second_char = info['second_char']
        second_char_pinyin = info['second_char_pinyin']

        pronunciation = None
        reason = ""
        method_used = ""

        # ä¼˜å…ˆçº§1ï¼šæ£€æŸ¥é¦–å­—åœ¨è”åˆé›†åˆä¸­æ˜¯å¦ç‹¬ä¸€æ— äºŒ
        if first_char_pinyin and union_pinyin_counter[first_char_pinyin] == 1:
            pronunciation = first_char
            reason = "é¦–å­—è¯»éŸ³åœ¨è”åˆé›†åˆä¸­ç‹¬ä¸€æ— äºŒ"
            method_used = "é¦–å­—"

        # ä¼˜å…ˆçº§2ï¼šæ£€æŸ¥ç¬¬äºŒä¸ªå­—åœ¨è”åˆé›†åˆä¸­æ˜¯å¦ç‹¬ä¸€æ— äºŒ
        elif second_char_pinyin and union_pinyin_counter[second_char_pinyin] == 1:
            pronunciation = second_char
            reason = "ç¬¬äºŒä¸ªå­—è¯»éŸ³åœ¨è”åˆé›†åˆä¸­ç‹¬ä¸€æ— äºŒ"
            method_used = "ç¬¬äºŒä¸ªå­—"

        # ä¼˜å…ˆçº§3ï¼šä½¿ç”¨å®Œæ•´è¯æ±‡
        else:
            pronunciation = word
            if first_char_pinyin and second_char_pinyin:
                reason = f"é¦–å­—å’Œç¬¬äºŒä¸ªå­—è¯»éŸ³éƒ½ä¸ç‹¬ç‰¹(é¦–å­—:{union_pinyin_counter[first_char_pinyin]}æ¬¡, ç¬¬äºŒä¸ªå­—:{union_pinyin_counter[second_char_pinyin]}æ¬¡)"
            elif first_char_pinyin:
                reason = f"é¦–å­—è¯»éŸ³ä¸ç‹¬ç‰¹({union_pinyin_counter[first_char_pinyin]}æ¬¡), ç¬¬äºŒä¸ªå­—æ— è¯»éŸ³"
            else:
                reason = "é¦–å­—è¯»éŸ³ä¸ç‹¬ç‰¹, ç¬¬äºŒä¸ªå­—æ— è¯»éŸ³"
            method_used = "å®Œæ•´è¯"

        pronunciation_results.append({
            'word': word,
            'first_char': first_char,
            'first_char_pinyin': first_char_pinyin,
            'second_char': second_char,
            'second_char_pinyin': second_char_pinyin,
            'pronunciation': pronunciation,
            'method_used': method_used,
            'reason': reason,
            'first_union_count': union_pinyin_counter.get(first_char_pinyin, 0),
            'second_union_count': union_pinyin_counter.get(second_char_pinyin, 0) if second_char_pinyin else 0
        })

    # ç»Ÿè®¡ä¿¡æ¯
    total_words = len(pronunciation_results)
    first_char_count = sum(1 for result in pronunciation_results if result['method_used'] == 'é¦–å­—')
    second_char_count = sum(1 for result in pronunciation_results if result['method_used'] == 'ç¬¬äºŒä¸ªå­—')
    full_word_count = sum(1 for result in pronunciation_results if result['method_used'] == 'å®Œæ•´è¯')

    stats = {
        'total_words': total_words,
        'first_char_count': first_char_count,
        'second_char_count': second_char_count,
        'full_word_count': full_word_count,
        'union_pinyin_count': len(union_pinyin_counter),
        'single_pinyin_count': len(all_single_pinyin),
        'first_pinyin_frequency': dict(first_pinyin_counter),
        'second_pinyin_frequency': dict(second_pinyin_counter),
        'single_pinyin_frequency': dict(single_pinyin_counter),
        'union_pinyin_frequency': dict(union_pinyin_counter)
    }

    return pronunciation_results, stats, classification


# ==================== ç»“æœå±•ç¤ºå‡½æ•° ====================

def print_method1_results(pronunciation_results, stats):
    """
    æ‰“å°ç¬¬ä¸€ç±»æ–¹æ¡ˆçš„åˆ†æç»“æœ
    """
    print("\n" + "=" * 80)
    print("ã€ç¬¬ä¸€ç±»æ–¹æ¡ˆã€‘è¯è¯­è¯»éŸ³åˆ†æç»“æœ")
    print("=" * 80)

    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»è¯æ•°: {stats['total_words']}")
    print(
        f"   ä½¿ç”¨é¦–å­—ä½œä¸ºè¯»éŸ³çš„è¯æ•°: {stats['unique_pronunciation_count']} ({stats['unique_pronunciation_count'] / stats['total_words'] * 100:.1f}%)")
    print(
        f"   ä½¿ç”¨å®Œæ•´è¯ä½œä¸ºè¯»éŸ³çš„è¯æ•°: {stats['repeated_pronunciation_count']} ({stats['repeated_pronunciation_count'] / stats['total_words'] * 100:.1f}%)")
    print(f"   å”¯ä¸€é¦–å­—è¯»éŸ³æ•°: {stats['unique_pinyin_count']}")
    print(f"   é‡å¤é¦–å­—è¯»éŸ³ç§ç±»æ•°: {stats['repeated_pinyin_count']}")

    # æ˜¾ç¤ºä½¿ç”¨é¦–å­—ä½œä¸ºè¯»éŸ³çš„è¯è¯­ç¤ºä¾‹
    unique_examples = [result for result in pronunciation_results if result['is_unique']]
    if unique_examples:
        print(f"\nâœ¨ ä½¿ç”¨é¦–å­—ä½œä¸ºè¯»éŸ³çš„è¯è¯­ç¤ºä¾‹ (å‰15ä¸ª):")
        for i, result in enumerate(unique_examples[:15]):
            print(f"   {i + 1:2d}. {result['word']:8s} â†’ {result['pronunciation']} ({result['first_char_pinyin']})")

    # æ˜¾ç¤ºä½¿ç”¨å®Œæ•´è¯ä½œä¸ºè¯»éŸ³çš„è¯è¯­ç¤ºä¾‹
    repeated_examples = [result for result in pronunciation_results if not result['is_unique']]
    if repeated_examples:
        print(f"\nğŸ” ä½¿ç”¨å®Œæ•´è¯ä½œä¸ºè¯»éŸ³çš„è¯è¯­ç¤ºä¾‹ (å‰15ä¸ª):")
        for i, result in enumerate(repeated_examples[:15]):
            print(
                f"   {i + 1:2d}. {result['word']:8s} â†’ {result['pronunciation']} ({result['first_char_pinyin']}, {result['repeat_count']}æ¬¡é‡å¤)")


def print_method2_results(pronunciation_results, stats):
    """
    æ‰“å°ç¬¬äºŒç±»æ–¹æ¡ˆçš„åˆ†æç»“æœ
    """
    print("\n" + "=" * 80)
    print("ã€ç¬¬äºŒç±»æ–¹æ¡ˆã€‘è¯è¯­è¯»éŸ³åˆ†æç»“æœ")
    print("=" * 80)

    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»è¯æ•°: {stats['total_words']}")
    print(
        f"   ä½¿ç”¨é¦–å­—ä½œä¸ºè¯»éŸ³çš„è¯æ•°: {stats['first_char_count']} ({stats['first_char_count'] / stats['total_words'] * 100:.1f}%)")
    print(
        f"   ä½¿ç”¨ç¬¬äºŒä¸ªå­—ä½œä¸ºè¯»éŸ³çš„è¯æ•°: {stats['second_char_count']} ({stats['second_char_count'] / stats['total_words'] * 100:.1f}%)")
    print(
        f"   ä½¿ç”¨å®Œæ•´è¯ä½œä¸ºè¯»éŸ³çš„è¯æ•°: {stats['full_word_count']} ({stats['full_word_count'] / stats['total_words'] * 100:.1f}%)")
    print(f"   è”åˆé›†åˆå”¯ä¸€è¯»éŸ³æ•°: {stats['union_pinyin_count']}")

    # æŒ‰æ–¹æ³•åˆ†ç±»æ˜¾ç¤ºç¤ºä¾‹
    method_examples = {
        'é¦–å­—': [result for result in pronunciation_results if result['method_used'] == 'é¦–å­—'],
        'ç¬¬äºŒä¸ªå­—': [result for result in pronunciation_results if result['method_used'] == 'ç¬¬äºŒä¸ªå­—'],
        'å®Œæ•´è¯': [result for result in pronunciation_results if result['method_used'] == 'å®Œæ•´è¯']
    }

    for method, examples in method_examples.items():
        if examples:
            print(f"\nğŸ¯ ä½¿ç”¨{method}ä½œä¸ºè¯»éŸ³çš„è¯è¯­ç¤ºä¾‹ (å‰10ä¸ª):")
            for i, result in enumerate(examples[:10]):
                if method == 'é¦–å­—':
                    print(
                        f"   {i + 1:2d}. {result['word']:8s} â†’ {result['pronunciation']} (é¦–å­—:{result['first_char_pinyin']}, è”åˆé›†åˆä¸­1æ¬¡)")
                elif method == 'ç¬¬äºŒä¸ªå­—':
                    print(
                        f"   {i + 1:2d}. {result['word']:8s} â†’ {result['pronunciation']} (ç¬¬äºŒä¸ªå­—:{result['second_char_pinyin']}, è”åˆé›†åˆä¸­1æ¬¡)")
                else:
                    print(
                        f"   {i + 1:2d}. {result['word']:8s} â†’ {result['pronunciation']} (é¦–å­—:{result['first_union_count']}æ¬¡, ç¬¬äºŒä¸ªå­—:{result['second_union_count']}æ¬¡)")


def print_special_words_analysis(special_analysis):
    """
    æ‰“å°ç‰¹æ®Šè¯æ±‡åˆ†æç»“æœ
    """
    print("\n" + "=" * 80)
    print("ã€ç‰¹æ®Šè¯æ±‡åˆ†æã€‘")
    print("=" * 80)

    # å•å­—æ±‰å­—è¯æ±‡
    if special_analysis['single_chinese_analysis']:
        print(f"\nğŸ“ å•å­—æ±‰å­—è¯æ±‡ ({len(special_analysis['single_chinese_analysis'])}ä¸ª):")
        for i, item in enumerate(special_analysis['single_chinese_analysis'][:20]):
            print(f"   {i + 1:2d}. {item['word']} â†’ {item['pronunciation']} ({item['pinyin']})")
        if len(special_analysis['single_chinese_analysis']) > 20:
            print(f"   ... è¿˜æœ‰{len(special_analysis['single_chinese_analysis']) - 20}ä¸ª")

    # è‹±æ–‡è¯æ±‡
    if special_analysis['english_words_analysis']:
        print(f"\nğŸ”¤ è‹±æ–‡è¯æ±‡ ({len(special_analysis['english_words_analysis'])}ä¸ª):")
        for i, item in enumerate(special_analysis['english_words_analysis'][:20]):
            print(f"   {i + 1:2d}. {item['word']} â†’ {item['pronunciation']}")
        if len(special_analysis['english_words_analysis']) > 20:
            print(f"   ... è¿˜æœ‰{len(special_analysis['english_words_analysis']) - 20}ä¸ª")

    # ä¸­è‹±æ··åˆè¯æ±‡
    if special_analysis['mixed_words_analysis']:
        print(f"\nğŸ”€ ä¸­è‹±æ··åˆè¯æ±‡ ({len(special_analysis['mixed_words_analysis'])}ä¸ª):")
        for i, item in enumerate(special_analysis['mixed_words_analysis'][:20]):
            chinese_part = f", æ±‰å­—éƒ¨åˆ†:{item['chinese_part']}" if item['chinese_part'] else ""
            pinyin_part = f", æ‹¼éŸ³:{item['pinyin']}" if item['pinyin'] else ""
            print(f"   {i + 1:2d}. {item['word']} â†’ {item['pronunciation']}{chinese_part}{pinyin_part}")
        if len(special_analysis['mixed_words_analysis']) > 20:
            print(f"   ... è¿˜æœ‰{len(special_analysis['mixed_words_analysis']) - 20}ä¸ª")

    # å…¶ä»–ç‰¹æ®Šè¯æ±‡
    if special_analysis['other_special_analysis']:
        print(f"\nğŸ”£ å…¶ä»–ç‰¹æ®Šè¯æ±‡ ({len(special_analysis['other_special_analysis'])}ä¸ª):")
        for i, item in enumerate(special_analysis['other_special_analysis'][:20]):
            print(f"   {i + 1:2d}. {item['word']} â†’ {item['pronunciation']}")
        if len(special_analysis['other_special_analysis']) > 20:
            print(f"   ... è¿˜æœ‰{len(special_analysis['other_special_analysis']) - 20}ä¸ª")


def print_pronunciation_list(pronunciation_results, method_name):
    """
    æ‰“å°è¯»éŸ³åˆ—è¡¨
    """
    print(f"\nğŸ“ ã€{method_name}ã€‘æ¯ä¸ªè¯çš„è¯»éŸ³è¡¨ç¤ºï¼ˆæ±‰å­—å½¢å¼ï¼‰:")
    print(f"{'åºå·':<4} {'åŸè¯':<10} {'è¯»éŸ³':<10} {'æ–¹æ³•':<8}")
    print("-" * 40)

    for i, result in enumerate(pronunciation_results[:30]):  # åªæ˜¾ç¤ºå‰30ä¸ª
        method = result.get('method_used', 'é¦–å­—' if result.get('is_unique') else 'å®Œæ•´è¯')
        print(f"{i + 1:<4} {result['word']:<10} {result['pronunciation']:<10} {method:<8}")

    if len(pronunciation_results) > 30:
        print(f"... è¿˜æœ‰{len(pronunciation_results) - 30}ä¸ª")

    # æå–çº¯è¯»éŸ³åˆ—è¡¨
    pronunciation_list = [result['pronunciation'] for result in pronunciation_results]

    print(f"\nğŸ“‹ ã€{method_name}ã€‘çº¯è¯»éŸ³æ•°ç»„ (å‰30ä¸ª):")
    print(pronunciation_list[:30])

    return pronunciation_list


def compare_methods(method1_results, method2_results):
    """
    å¯¹æ¯”ä¸¤ç§æ–¹æ¡ˆçš„ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ã€æ–¹æ¡ˆå¯¹æ¯”åˆ†æã€‘")
    print("=" * 80)

    method1_pronunciations = [result['pronunciation'] for result in method1_results]
    method2_pronunciations = [result['pronunciation'] for result in method2_results]

    # ç»Ÿè®¡å·®å¼‚
    different_count = 0
    same_count = 0
    differences = []

    for i, (m1_result, m2_result) in enumerate(zip(method1_results, method2_results)):
        if m1_result['pronunciation'] != m2_result['pronunciation']:
            different_count += 1
            differences.append({
                'index': i,
                'word': m1_result['word'],
                'method1_pronunciation': m1_result['pronunciation'],
                'method2_pronunciation': m2_result['pronunciation'],
                'method1_reason': m1_result['reason'],
                'method2_reason': m2_result['reason']
            })
        else:
            same_count += 1

    print(f"\nğŸ“Š å¯¹æ¯”ç»Ÿè®¡:")
    print(f"   æ€»è¯æ•°: {len(method1_results)}")
    print(f"   è¯»éŸ³ç›¸åŒçš„è¯æ•°: {same_count} ({same_count / len(method1_results) * 100:.1f}%)")
    print(f"   è¯»éŸ³ä¸åŒçš„è¯æ•°: {different_count} ({different_count / len(method1_results) * 100:.1f}%)")

    if differences:
        print(f"\nğŸ”„ è¯»éŸ³ä¸åŒçš„è¯è¯­ç¤ºä¾‹ (å‰15ä¸ª):")
        print(f"{'åºå·':<4} {'è¯è¯­':<8} {'æ–¹æ¡ˆ1è¯»éŸ³':<10} {'æ–¹æ¡ˆ2è¯»éŸ³':<10}")
        print("-" * 50)
        for i, diff in enumerate(differences[:15]):
            print(
                f"{i + 1:<4} {diff['word']:<8} {diff['method1_pronunciation']:<10} {diff['method2_pronunciation']:<10}")

    return differences


def save_comprehensive_results(method1_results, method1_stats, method2_results, method2_stats,
                               differences, special_analysis, classification,
                               filename="comprehensive_pronunciation_results.txt"):
    """
    ä¿å­˜ç»¼åˆç»“æœåˆ°æ–‡ä»¶ï¼ŒåŒ…å«ç‰¹æ®Šè¯æ±‡åˆ†æ
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("è¯è¯­è¯»éŸ³åˆ†æç»¼åˆç»“æœ\n")
            f.write("=" * 60 + "\n\n")

            # è¯æ±‡åˆ†ç±»ç»Ÿè®¡
            f.write("ã€è¯æ±‡åˆ†ç±»ç»Ÿè®¡ã€‘\n")
            f.write(f"æ­£å¸¸æ±‰å­—è¯æ±‡: {len(classification['normal_chinese'])}\n")
            f.write(f"å•å­—æ±‰å­—è¯æ±‡: {len(classification['single_chinese'])}\n")
            f.write(f"è‹±æ–‡è¯æ±‡: {len(classification['english_words'])}\n")
            f.write(f"ä¸­è‹±æ··åˆè¯æ±‡: {len(classification['mixed_words'])}\n")
            f.write(f"å…¶ä»–ç‰¹æ®Šè¯æ±‡: {len(classification['special_words'])}\n\n")

            # ç¬¬ä¸€ç±»æ–¹æ¡ˆç»“æœ
            f.write("ã€ç¬¬ä¸€ç±»æ–¹æ¡ˆç»“æœã€‘(ä»…å¤„ç†æ­£å¸¸æ±‰å­—è¯æ±‡)\n")
            f.write(f"æ€»è¯æ•°: {method1_stats['total_words']}\n")
            f.write(
                f"ä½¿ç”¨é¦–å­—: {method1_stats['unique_pronunciation_count']} ({method1_stats['unique_pronunciation_count'] / method1_stats['total_words'] * 100:.1f}%)\n")
            f.write(
                f"ä½¿ç”¨å®Œæ•´è¯: {method1_stats['repeated_pronunciation_count']} ({method1_stats['repeated_pronunciation_count'] / method1_stats['total_words'] * 100:.1f}%)\n\n")

            # ç¬¬äºŒç±»æ–¹æ¡ˆç»“æœ
            f.write("ã€ç¬¬äºŒç±»æ–¹æ¡ˆç»“æœã€‘(ä»…å¤„ç†æ­£å¸¸æ±‰å­—è¯æ±‡)\n")
            f.write(f"æ€»è¯æ•°: {method2_stats['total_words']}\n")
            f.write(
                f"ä½¿ç”¨é¦–å­—: {method2_stats['first_char_count']} ({method2_stats['first_char_count'] / method2_stats['total_words'] * 100:.1f}%)\n")
            f.write(
                f"ä½¿ç”¨ç¬¬äºŒä¸ªå­—: {method2_stats['second_char_count']} ({method2_stats['second_char_count'] / method2_stats['total_words'] * 100:.1f}%)\n")
            f.write(
                f"ä½¿ç”¨å®Œæ•´è¯: {method2_stats['full_word_count']} ({method2_stats['full_word_count'] / method2_stats['total_words'] * 100:.1f}%)\n\n")

            # å¯¹æ¯”ç»“æœ
            f.write("ã€æ–¹æ¡ˆå¯¹æ¯”ã€‘\n")
            f.write(f"è¯»éŸ³ç›¸åŒçš„è¯æ•°: {len(method1_results) - len(differences)}\n")
            f.write(f"è¯»éŸ³ä¸åŒçš„è¯æ•°: {len(differences)}\n\n")

            # ==================== ç‰¹æ®Šè¯æ±‡åˆ†æ ====================
            f.write("ã€ç‰¹æ®Šè¯æ±‡è¯»éŸ³åˆ†æã€‘\n")
            f.write("=" * 40 + "\n\n")

            # å•å­—æ±‰å­—è¯æ±‡
            if special_analysis['single_chinese_analysis']:
                f.write(f"å•å­—æ±‰å­—è¯æ±‡ ({len(special_analysis['single_chinese_analysis'])}ä¸ª):\n")
                f.write("åºå·\tè¯æ±‡\tè¯»éŸ³\tæ‹¼éŸ³\tè¯´æ˜\n")
                for i, item in enumerate(special_analysis['single_chinese_analysis']):
                    pinyin_str = item['pinyin'] if item['pinyin'] else "N/A"
                    f.write(f"{i + 1}\t{item['word']}\t{item['pronunciation']}\t{pinyin_str}\t{item['reason']}\n")
                f.write("\n")

            # è‹±æ–‡è¯æ±‡
            if special_analysis['english_words_analysis']:
                f.write(f"è‹±æ–‡è¯æ±‡ ({len(special_analysis['english_words_analysis'])}ä¸ª):\n")
                f.write("åºå·\tè¯æ±‡\tè¯»éŸ³\tè¯´æ˜\n")
                for i, item in enumerate(special_analysis['english_words_analysis']):
                    f.write(f"{i + 1}\t{item['word']}\t{item['pronunciation']}\t{item['reason']}\n")
                f.write("\n")

            # ä¸­è‹±æ··åˆè¯æ±‡
            if special_analysis['mixed_words_analysis']:
                f.write(f"ä¸­è‹±æ··åˆè¯æ±‡ ({len(special_analysis['mixed_words_analysis'])}ä¸ª):\n")
                f.write("åºå·\tè¯æ±‡\tè¯»éŸ³\tæ±‰å­—éƒ¨åˆ†\tæ±‰å­—æ‹¼éŸ³\tè¯´æ˜\n")
                for i, item in enumerate(special_analysis['mixed_words_analysis']):
                    chinese_part = item['chinese_part'] if item['chinese_part'] else "æ— "
                    pinyin_str = item['pinyin'] if item['pinyin'] else "N/A"
                    f.write(
                        f"{i + 1}\t{item['word']}\t{item['pronunciation']}\t{chinese_part}\t{pinyin_str}\t{item['reason']}\n")
                f.write("\n")

            # å…¶ä»–ç‰¹æ®Šè¯æ±‡
            if special_analysis['other_special_analysis']:
                f.write(f"å…¶ä»–ç‰¹æ®Šè¯æ±‡ ({len(special_analysis['other_special_analysis'])}ä¸ª):\n")
                f.write("åºå·\tè¯æ±‡\tè¯»éŸ³\tè¯´æ˜\n")
                for i, item in enumerate(special_analysis['other_special_analysis']):
                    f.write(f"{i + 1}\t{item['word']}\t{item['pronunciation']}\t{item['reason']}\n")
                f.write("\n")

            # ==================== æ­£å¸¸è¯æ±‡è¯¦ç»†æ•°æ® ====================
            f.write("ã€æ­£å¸¸æ±‰å­—è¯æ±‡è¯¦ç»†æ•°æ®ã€‘\n")
            f.write("=" * 40 + "\n")
            f.write("åºå·\tåŸè¯\tæ–¹æ¡ˆ1è¯»éŸ³\tæ–¹æ¡ˆ2è¯»éŸ³\tæ–¹æ¡ˆ1æ–¹æ³•\tæ–¹æ¡ˆ2æ–¹æ³•\n")
            for i, (m1_result, m2_result) in enumerate(zip(method1_results, method2_results)):
                m1_method = "é¦–å­—" if m1_result.get('is_unique') else "å®Œæ•´è¯"
                m2_method = m2_result.get('method_used', '')
                f.write(
                    f"{i + 1}\t{m1_result['word']}\t{m1_result['pronunciation']}\t{m2_result['pronunciation']}\t{m1_method}\t{m2_method}\n")

            # ==================== å®Œæ•´è¯»éŸ³åˆ—è¡¨ ====================
            f.write("\nã€å®Œæ•´è¯»éŸ³åˆ—è¡¨ã€‘\n")
            f.write("=" * 40 + "\n")

            # åˆå¹¶æ‰€æœ‰è¯»éŸ³ï¼ˆæ­£å¸¸è¯æ±‡ + ç‰¹æ®Šè¯æ±‡ï¼‰
            all_pronunciations_m1 = []
            all_pronunciations_m2 = []

            # æ·»åŠ æ­£å¸¸è¯æ±‡çš„è¯»éŸ³
            all_pronunciations_m1.extend([result['pronunciation'] for result in method1_results])
            all_pronunciations_m2.extend([result['pronunciation'] for result in method2_results])

            # æ·»åŠ ç‰¹æ®Šè¯æ±‡çš„è¯»éŸ³
            for category in ['single_chinese_analysis', 'english_words_analysis', 'mixed_words_analysis',
                             'other_special_analysis']:
                for item in special_analysis[category]:
                    all_pronunciations_m1.append(item['pronunciation'])
                    all_pronunciations_m2.append(item['pronunciation'])

            f.write("æ–¹æ¡ˆ1å®Œæ•´è¯»éŸ³åˆ—è¡¨:\n")
            for i, pronunciation in enumerate(all_pronunciations_m1):
                f.write(f"{i + 1}. {pronunciation}\n")

            f.write("\næ–¹æ¡ˆ2å®Œæ•´è¯»éŸ³åˆ—è¡¨:\n")
            for i, pronunciation in enumerate(all_pronunciations_m2):
                f.write(f"{i + 1}. {pronunciation}\n")

        print(f"\nğŸ’¾ ç»¼åˆç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")

    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å‡ºé”™: {e}")


def main():
    """
    ä¸»å‡½æ•°
    """
    # æ–‡ä»¶è·¯å¾„å’Œå‚æ•°è®¾ç½®
    file_path = 'ci-test.xlsx'  # è¯·ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
    sheet_name = 'ci-test'

    print("ğŸš€ å¼€å§‹è¯»å–Excelæ–‡ä»¶...")

    # è¯»å–Excelæ•°æ®ï¼ˆè‡ªåŠ¨æ’é™¤"-"ï¼‰
    words = read_excel_range_by_columns(file_path, sheet_name, 'B2', 'Y25')

    if not words:
        print("âŒ æ²¡æœ‰è¯»å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå·¥ä½œè¡¨åç§°")
        return

    print(f"âœ… æˆåŠŸè¯»å–åˆ° {len(words)} ä¸ªæœ‰æ•ˆè¯è¯­ï¼ˆå·²æ’é™¤'-'ï¼‰")

    # ==================== è¿è¡Œç¬¬ä¸€ç±»æ–¹æ¡ˆ ====================
    print("\n" + "ğŸ”µ" * 20 + " ç¬¬ä¸€ç±»æ–¹æ¡ˆ " + "ğŸ”µ" * 20)
    method1_results, method1_stats, classification = determine_word_pronunciation_method1(words)

    if not method1_results:
        print("âŒ ç¬¬ä¸€ç±»æ–¹æ¡ˆæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ±‰å­—è¯è¯­")
        return

    print_method1_results(method1_results, method1_stats)
    method1_pronunciation_list = print_pronunciation_list(method1_results, "ç¬¬ä¸€ç±»æ–¹æ¡ˆ")

    # ==================== è¿è¡Œç¬¬äºŒç±»æ–¹æ¡ˆ ====================
    print("\n" + "ğŸŸ¢" * 20 + " ç¬¬äºŒç±»æ–¹æ¡ˆ " + "ğŸŸ¢" * 20)
    method2_results, method2_stats, _ = determine_word_pronunciation_method2(words)

    if not method2_results:
        print("âŒ ç¬¬äºŒç±»æ–¹æ¡ˆæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ±‰å­—è¯è¯­")
        return

    print_method2_results(method2_results, method2_stats)
    method2_pronunciation_list = print_pronunciation_list(method2_results, "ç¬¬äºŒç±»æ–¹æ¡ˆ")

    # ==================== ç‰¹æ®Šè¯æ±‡åˆ†æ ====================
    print("\n" + "ğŸŸ¡" * 20 + " ç‰¹æ®Šè¯æ±‡åˆ†æ " + "ğŸŸ¡" * 20)
    special_analysis = analyze_special_words(classification)
    print_special_words_analysis(special_analysis)

    # ==================== å¯¹æ¯”åˆ†æ ====================
    differences = compare_methods(method1_results, method2_results)

    # ä¿å­˜ç»¼åˆç»“æœï¼ˆåŒ…å«ç‰¹æ®Šè¯æ±‡ï¼‰
    save_comprehensive_results(method1_results, method1_stats, method2_results, method2_stats,
                               differences, special_analysis, classification)

    # è¿”å›ç»“æœ
    return {
        'words': words,
        'classification': classification,
        'method1_pronunciations': method1_pronunciation_list,
        'method2_pronunciations': method2_pronunciation_list,
        'method1_results': method1_results,
        'method2_results': method2_results,
        'method1_stats': method1_stats,
        'method2_stats': method2_stats,
        'special_analysis': special_analysis,
        'differences': differences
    }


if __name__ == "__main__":
    # å®‰è£…ä¾èµ–æç¤º
    print("ğŸ“¦ è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install pandas openpyxl pypinyin")
    print()

    # è¿è¡Œè„šæœ¬
    results = main()

    if results:
        print(f"\nğŸ‰ ç»¼åˆåˆ†æå®Œæˆï¼")
        print(f"   - æ€»è¯è¯­æ•°: {len(results['words'])}")
        print(f"   - æ­£å¸¸æ±‰å­—è¯è¯­: {len(results['classification']['normal_chinese'])}")
        print(f"   - å•å­—æ±‰å­—è¯è¯­: {len(results['classification']['single_chinese'])}")
        print(f"   - è‹±æ–‡è¯è¯­: {len(results['classification']['english_words'])}")
        print(f"   - ä¸­è‹±æ··åˆè¯è¯­: {len(results['classification']['mixed_words'])}")
        print(f"   - å…¶ä»–ç‰¹æ®Šè¯è¯­: {len(results['classification']['special_words'])}")
        print(f"   - ä¸¤æ–¹æ¡ˆå·®å¼‚è¯è¯­æ•°: {len(results['differences'])}")

        # è®¡ç®—å®Œæ•´è¯»éŸ³æ•°ç»„é•¿åº¦ï¼ˆåŒ…å«ç‰¹æ®Šè¯æ±‡ï¼‰
        total_special = sum(len(results['special_analysis'][key]) for key in results['special_analysis'])
        total_pronunciations = len(results['method1_pronunciations']) + total_special

        print(f"\nğŸ“Š å®Œæ•´è¯»éŸ³æ•°ç»„:")
        print(f"   - ç¬¬ä¸€ç±»æ–¹æ¡ˆå®Œæ•´è¯»éŸ³æ•°ç»„é•¿åº¦: {total_pronunciations}")
        print(f"   - ç¬¬äºŒç±»æ–¹æ¡ˆå®Œæ•´è¯»éŸ³æ•°ç»„é•¿åº¦: {total_pronunciations}")